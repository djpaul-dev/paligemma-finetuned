{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, Image, ImageFont\n",
    "\n",
    "def code_to_img(text):\n",
    "    img = Image.new('RGB', (400, 800))\n",
    "    d = ImageDraw.Draw(img)\n",
    "    # Specify the font size\n",
    "    d.text((20, 20), text, fill=(255, 255, 255))\n",
    "    return img\n",
    "\n",
    "# code_to_img(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['code', 'repo_name', 'path', 'language', 'license', 'size'],\n",
       "    n_shards: 1126\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"codeparrot/github-code\", streaming=True, split=\"train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 4900 4800 4700 4600 4500 4400 4300 4200 4100 4000 3900 3800 3700 3600 3500 3400 3300 3200 3100 3000 2900 2800 2700 2600 2500 2400 2300 2200 2100 2000 1900 1800 1700 1600 1500 1400 1300 1200 1100 1000 900 800 700 600 500 400 300 200 100 0 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 5000, 5000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = []  # codes\n",
    "images = []  # images of the codes\n",
    "questions = []  # questions\n",
    "\n",
    "count = 5000\n",
    "for row in ds:\n",
    "    if count % 100 == 0:\n",
    "        print(count, end=\" \")\n",
    "    if count == 0:\n",
    "        break\n",
    "    code = row[\"code\"][:1700]\n",
    "    responses.append(code)\n",
    "    images.append(code_to_img(code))\n",
    "    questions.append(\"What's in this image?\")\n",
    "    count -= 1\n",
    "\n",
    "len(responses), len(images), len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download image_url to PIL Image\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# image_url = \"https://i.ytimg.com/vi/71Z-Jpnm3D4/maxresdefault.jpg\"\n",
    "\n",
    "# def url_to_PIL(image_url):\n",
    "#     img = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
    "#     return img\n",
    "\n",
    "# print(url_to_PIL(image_url))\n",
    "\n",
    "# # create dataset with custom columns\n",
    "# from datasets import Dataset\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# questions = [\n",
    "#     \"What algorithm is portrayed in this image?\",\n",
    "#     \"What is this image about?\",\n",
    "#     \"What type of tree is in this image?\",\n",
    "#     \"What algorithm is this diagram about?\",\n",
    "#     \"What algorithm is this code using?\"\n",
    "# ]\n",
    "\n",
    "# urls = [\n",
    "#     \"https://i.ytimg.com/vi/71Z-Jpnm3D4/maxresdefault.jpg\",\n",
    "#     \"https://i.ytimg.com/vi/jsmMtJpPnhU/maxresdefault.jpg\",\n",
    "#     \"https://web.cecs.pdx.edu/~sheard/course/Cs163/Graphics/FullBinary.jpg\",\n",
    "#     \"https://i.ytimg.com/vi/XWo_Q0YiNY4/maxresdefault.jpg\",\n",
    "#     \"https://www.andrew.cmu.edu/course/15-121/lectures/Sorting%20Algorithms/pix/bubbleSort.bmp\"\n",
    "# ]\n",
    "\n",
    "# responses = [\n",
    "#     \"Dijkstra's Algorithm\",\n",
    "#     \"Prim's Minimum Spanning Tree Algorithm\",\n",
    "#     \"Binary Tree\",\n",
    "#     \"Kosaraju's Algorithm\",\n",
    "#     \"Sorting Algorithm\",\n",
    "# ]\n",
    "\n",
    "# images = []\n",
    "# for u in urls:\n",
    "#     print(u)\n",
    "#     images.append(url_to_PIL(u))\n",
    "\n",
    "# len(questions), len(urls), len(images), len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"What's in this image?\", 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=400x800 at 0x7FF15FD98C40>, 'multiple_choice_answer': \"import { expect } from 'chai';\\n\\nimport { Curry } from './curry';\\n\\ndescribe('curry', () => {\\n  it('should curry the method with default arity', () => {\\n    class MyClass {\\n      @Curry()\\n      add(a: any, b?: any) {\\n        return a + b;\\n      }\\n    }\\n\\n    const myClass = new MyClass();\\n    const add5 = myClass.add(5);\\n\\n    expect(add5).to.be.an.instanceOf(Function);\\n    expect(add5(10)).to.equal(15);\\n  });\\n\\n  it('should curry the method with default arity (paramless)', () => {\\n    class MyClass {\\n      @Curry\\n      add(a: any, b?: any) {\\n        return a + b;\\n      }\\n    }\\n\\n    const myClass = new MyClass();\\n    const add5 = myClass.add(5);\\n\\n    expect(add5).to.be.an.instanceOf(Function);\\n    expect(add5(10)).to.equal(15);\\n  });\\n\\n  it('should curry the method with fixed arity', () => {\\n    class MyClass {\\n      @Curry(2)\\n      add(a: any, b?: any, c?: any) {\\n        return a + b * c;\\n      }\\n    }\\n\\n    const myClass = new MyClass();\\n    const add5 = myClass.add(5);\\n\\n    expect(add5).to.be.an.instanceOf(Function);\\n    expect(add5(10, 2)).to.equal(25);\\n  });\\n\\n  it('should retain the class context', () => {\\n    class MyClass {\\n      value = 10;\\n\\n      @Curry()\\n      add(a: any, b?: any): any {\\n        return (a + b) * this.value;\\n      }\\n    }\\n\\n    const myClass = new MyClass();\\n    const add5 = myClass.add(5);\\n\\n    expect(add5(2)).to.equal(70);\\n  });\\n});\\n\"}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "data = {\"question\": questions, \"image\": images, \"multiple_choice_answer\": responses}\n",
    "ds = Dataset.from_dict(data)\n",
    "split_ds = ds.train_test_split(test_size=0.2)\n",
    "train_ds = split_ds[\"train\"]\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_4bit_compute_type']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,298,816 || all params: 2,934,765,296 || trainable%: 0.3850\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 2. Load Model\n",
    "model_id = \"google/paligemma-3b-pt-224\"\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "device = \"cuda\"\n",
    "\n",
    "# Loading Quantised Model (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_type=torch.bfloat16\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id,\n",
    "                                                          quantization_config=bnb_config,\n",
    "                                                          device_map={\"\": 0})\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Ubuntu/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/Ubuntu/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 47:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.157000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.2626302490234376, metrics={'train_runtime': 2873.6138, 'train_samples_per_second': 2.784, 'train_steps_per_second': 0.174, 'total_flos': 8.456728320081274e+16, 'train_loss': 1.2626302490234376, 'epoch': 2.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Fine-tuning the model\n",
    "def collate_fn(examples):\n",
    "    texts = [\"answer \" + example[\"question\"] for example in examples]\n",
    "    labels = [example['multiple_choice_answer'] for example in examples]\n",
    "    images = [example[\"image\"].convert(\"RGB\") for example in examples]\n",
    "    tokens = processor(text=texts, images=images, suffix=labels,\n",
    "                       return_tensors=\"pt\", padding=\"longest\",\n",
    "                       tokenize_newline_separately=False)\n",
    "    tokens = tokens.to(torch.bfloat16).to(device)\n",
    "    return tokens\n",
    "\n",
    "args = TrainingArguments(\n",
    "    num_train_epochs=2,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=1e-6,\n",
    "    adam_beta2=0.999,\n",
    "    logging_steps=100,\n",
    "    optim=\"adamw_hf\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    push_to_hub=True,\n",
    "    save_total_limit=1,\n",
    "    output_dir=\"paligemma_CS\",\n",
    "    bf16=True,\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=collate_fn,\n",
    "    args=args\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/DJPAUL25/paligemma_CS/commit/30aff425a4ddff40ec100aeace45374171408c76', commit_message='DJPAUL25/paligemma_CS', commit_description='', oid='30aff425a4ddff40ec100aeace45374171408c76', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Save the model in HuggingFace\n",
    "trainer.push_to_hub('DJPAUL25/paligemma_CS')  # Ensure the pushed version is the final model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
