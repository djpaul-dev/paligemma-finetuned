{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in ./venv/lib/python3.10/site-packages (0.23.4)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.10/site-packages (10.3.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.10/site-packages (from huggingface-hub) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.10/site-packages (from huggingface-hub) (4.66.4)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from huggingface-hub) (3.15.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub) (2024.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->huggingface-hub) (2.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Ubuntu/llm_finetuning/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/Ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import PaliGemmaProcessor, AutoModelForPreTraining\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "token = os.getenv('HF_TOKEN')\n",
    "login(token=token)\n",
    "\n",
    "# Load PeftConfig and base model\n",
    "config = PeftConfig.from_pretrained(\"DJPAUL25/paligemma_CS\")\n",
    "base_model = AutoModelForPreTraining.from_pretrained(\"google/paligemma-3b-pt-224\")\n",
    "model_finetuned = PeftModel.from_pretrained(base_model, \"DJPAUL25/paligemma_CS\")\n",
    "\n",
    "# Loading PaliGemma Processor\n",
    "processor = PaliGemmaProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\n",
    "\n",
    "def do_inference(processor, input_text, input_image, model):\n",
    "    # Preprocess Inputs\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = processor(text=input_text, images=input_image, padding=\"longest\", do_convert_rgb=True, return_tensors=\"pt\").to(device)\n",
    "    model.to(device)\n",
    "    inputs = inputs.to(dtype=model.dtype)\n",
    "    \n",
    "    # Generating and Decoding Output\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=496)\n",
    "        \n",
    "    return processor.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's in this image?\n",
      "Bubble sort using Python\n",
      "def bubbleSort(list):\n",
      "for i in range(len(list)):\n",
      "for j in range(len(list) - 1, 1, -1):\n",
      "if list[j] < list[j - 1]:\n",
      "list[j], list[j - 1] = list[j - 1], list[j]\n",
      "return list\n",
      "if\n",
      "name == 'main__':\n",
      "List = [8, 4, 2, 6, 5, 7, 1, 9]\n",
      "print('Sorted List:', bubbleSort(list))\n",
      "#lccoding.com\n",
      "Sorted list: [1, 2, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Loading and Processing the Image\n",
    "input_text = \"What's in this image?\"\n",
    "img_url = \"https://media.licdn.com/dms/image/C5612AQEwy6oxw1jNfA/article-cover_image-shrink_720_1280/0/1652105355771?e=2147483647&v=beta&t=Dp48_xxmmrKerkFHiRr9md32I7ERU3dj1-RUGr4vYdg\"\n",
    "input_image = Image.open(requests.get(img_url, stream=True).raw)\n",
    "\n",
    "res = do_inference(processor, input_text, input_image, model_finetuned)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_4bit_compute_type']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "model_id = \"google/paligemma-3b-pt-224\"\n",
    "\n",
    "# Loading Quantised Model (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_type=torch.bfloat16\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id,\n",
    "                                                          quantization_config=bnb_config,\n",
    "                                                          device_map={\"\": 0})\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Ubuntu/llm_finetuning/venv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's in this image?\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "res = do_inference(processor, input_text, input_image, model)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
